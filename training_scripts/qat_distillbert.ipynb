{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AtO_E9vOzBbl"
      },
      "id": "AtO_E9vOzBbl"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sinhajiya/hf_emotion_classifier.git\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKIU9fqDzF42",
        "outputId": "34195fee-0dfc-4259-cda2-6b9a1080aadf"
      },
      "id": "PKIU9fqDzF42",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hf_emotion_classifier'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 49 (delta 17), reused 22 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (49/49), 1.26 MiB | 4.33 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a79e86b",
      "metadata": {
        "id": "7a79e86b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.ao.quantization as tq\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import sys\n",
        "project_root = os.path.abspath(\"/content/hf_emotion_classifier\")\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "from hf_data import *\n",
        "\n",
        "\n",
        "from torch.optim import AdamW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8d5fe5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b8d5fe5",
        "outputId": "082b640b-392a-4ef3-8b53-be21891ea83c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "\n",
        "finetune_model_dir = '/content/gdrive/MyDrive/ANLP_weights/distillbert_finetuned_model'\n",
        "tokenizer = AutoTokenizer.from_pretrained(finetune_model_dir)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(finetune_model_dir)\n",
        "model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b44fcf96",
      "metadata": {
        "id": "b44fcf96"
      },
      "outputs": [],
      "source": [
        "qat_qconfig = tq.get_default_qat_qconfig(\"fbgemm\")\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear):\n",
        "        module.qconfig = qat_qconfig\n",
        "    else:\n",
        "        module.qconfig = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db797a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db797a3",
        "outputId": "001b4d56-9cd6-4ca1-f22f-a6cdeb250322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1177552200.py:1: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  tq.prepare_qat(model, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model prepared\n"
          ]
        }
      ],
      "source": [
        "tq.prepare_qat(model, inplace=True)\n",
        "print(\"Model prepared\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c21b10",
      "metadata": {
        "id": "98c21b10"
      },
      "outputs": [],
      "source": [
        "train = emotions('train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd92a7a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd92a7a0",
        "outputId": "cf20e2bb-bb7f-49dd-a796-6fd88c6c76bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 16000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tokenized_train = train.map(lambda x: tokenize_batch(x, tokenizer), batched=True)\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "print(tokenized_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539edcb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "539edcb1",
        "outputId": "ddef7237-e53b-40ad-d2f5-f585949165e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 1000/1000 [03:03<00:00,  5.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Avg Loss = 0.1421 | Accuracy = 94.53%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 1000/1000 [03:02<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Avg Loss = 0.1071 | Accuracy = 95.21%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 1000/1000 [03:02<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Avg Loss = 0.0894 | Accuracy = 95.97%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 1000/1000 [03:02<00:00,  5.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Avg Loss = 0.0770 | Accuracy = 96.63%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 1000/1000 [03:02<00:00,  5.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Avg Loss = 0.0640 | Accuracy = 97.36%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    train_acc = correct / total * 100\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Avg Loss = {avg_loss:.4f} | Accuracy = {train_acc:.2f}%\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6abbd4b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6abbd4b1",
        "outputId": "32683759-0256-44b2-9660-220d9970278d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1824699939.py:4: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model_qat = tq.convert(model, inplace=False)\n"
          ]
        }
      ],
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "quantized_model_qat = tq.convert(model, inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb726e22",
      "metadata": {
        "id": "cb726e22"
      },
      "outputs": [],
      "source": [
        "torch.save(quantized_model_qat, r\"/content/gdrive/MyDrive/ANLP_weights/qat_distilbert.pt\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}