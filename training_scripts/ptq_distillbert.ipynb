{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1223f120",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14a1ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset, DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8da95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "from hf_data import *\n",
    "from metrics import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6f928d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97b3fb",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49df3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = emotions()\n",
    "train = emotions('train')\n",
    "val = emotions('validation')\n",
    "test = emotions('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebcbfd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b0e4e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbaddd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i2l': {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}, 'l2i': {'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}}\n"
     ]
    }
   ],
   "source": [
    "l = labels_and_ids()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b97e2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"distilbert/distilbert-base-uncased\"\n",
    "finetune_model_dir = r\"E:\\codes\\advanced_nlp\\hf_emotion_classifier\\models\\distillbert_finetuned_model\\distillbert_finetuned_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(finetune_model_dir, num_labels = 6, id2label=l['i2l'], label2id = l['l2i'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33880286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
      "{'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)\n",
    "print(model.config.label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1a493df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 10964.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding='max_length', max_length=max_length)\n",
    "# map datasets\n",
    "tokenized_train = train.map(lambda x: tokenize_batch(x), batched=True)\n",
    "tokenized_valid = val.map(lambda x: tokenize_batch(x), batched=True)\n",
    "tokenized_test  = test.map(lambda x: tokenize_batch(x), batched=True)\n",
    "# tokenized_calibration = calibration_data.map(tokenize_batch, batched=True)  \n",
    "\n",
    "# Set format for PyTorch compatibility\n",
    "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "tokenized_train.set_format(type=\"torch\", columns=cols)\n",
    "tokenized_valid.set_format(type=\"torch\", columns=cols)\n",
    "tokenized_test.set_format(type=\"torch\", columns=cols)\n",
    "# tokenized_calibration.set_format(type=\"torch\", columns=cols)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "892b6f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight                  shape=(30522, 768)  dtype=torch.float32\n",
      "distilbert.embeddings.position_embeddings.weight              shape=(512, 768)  dtype=torch.float32\n",
      "distilbert.embeddings.LayerNorm.weight                        shape=(768,)  dtype=torch.float32\n",
      "distilbert.embeddings.LayerNorm.bias                          shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.attention.q_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.attention.v_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight       shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias             shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias                  shape=(3072,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight                shape=(768, 3072)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias                  shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight       shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.attention.q_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.attention.q_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.attention.v_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.attention.v_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight       shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias             shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias                  shape=(3072,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight                shape=(768, 3072)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias                  shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight       shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.attention.q_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.attention.q_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.attention.v_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.attention.v_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight       shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias             shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias                  shape=(3072,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight                shape=(768, 3072)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias                  shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight       shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.attention.q_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.attention.q_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.attention.v_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.attention.v_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight       shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias             shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias                  shape=(3072,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight                shape=(768, 3072)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias                  shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight       shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.attention.q_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.attention.q_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.attention.v_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.attention.v_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight       shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias             shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias                  shape=(3072,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight                shape=(768, 3072)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias                  shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight       shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.attention.q_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.attention.q_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.attention.v_lin.weight         shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.attention.v_lin.bias           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight       shape=(768, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias         shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight           shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias             shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias                  shape=(3072,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight                shape=(768, 3072)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias                  shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight       shape=(768,)  dtype=torch.float32\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias         shape=(768,)  dtype=torch.float32\n",
      "pre_classifier.weight                                         shape=(768, 768)  dtype=torch.float32\n",
      "pre_classifier.bias                                           shape=(768,)  dtype=torch.float32\n",
      "classifier.weight                                             shape=(6, 768)  dtype=torch.float32\n",
      "classifier.bias                                               shape=(6,)  dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "print_model_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b8e874",
   "metadata": {},
   "source": [
    " currently all are in float 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d4f2e",
   "metadata": {},
   "source": [
    "## torchao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d08ec4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53596a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_on_cpu = model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83e35d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=6, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model_dynamic = torch.quantization.quantize_dynamic(\n",
    "    model_on_cpu,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "quantized_model_dynamic.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5c5f3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_lin): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (lin2): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=6, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model_dynamic.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "697eb873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('E:\\\\codes\\\\advanced_nlp\\\\hf_emotion_classifier\\\\models\\\\ptq_distilbert\\\\tokenizer_config.json',\n",
       " 'E:\\\\codes\\\\advanced_nlp\\\\hf_emotion_classifier\\\\models\\\\ptq_distilbert\\\\special_tokens_map.json',\n",
       " 'E:\\\\codes\\\\advanced_nlp\\\\hf_emotion_classifier\\\\models\\\\ptq_distilbert\\\\vocab.txt',\n",
       " 'E:\\\\codes\\\\advanced_nlp\\\\hf_emotion_classifier\\\\models\\\\ptq_distilbert\\\\added_tokens.json',\n",
       " 'E:\\\\codes\\\\advanced_nlp\\\\hf_emotion_classifier\\\\models\\\\ptq_distilbert\\\\tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "save_path = r\"E:\\codes\\advanced_nlp\\hf_emotion_classifier\\models\\ptq_distilbert\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "torch.save(quantized_model_dynamic, os.path.join(save_path, \"quantized_model.pt\"))\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a0801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
