{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1223f120",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a1ebe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DSE411/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import os\n",
    "import sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "from hf_data import *\n",
    "from metrics import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97b3fb",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49df3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = emotions()\n",
    "train = emotions('train')\n",
    "val = emotions('validation')\n",
    "test = emotions('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcbfd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b0e4e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbaddd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i2l': {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}, 'l2i': {'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}}\n"
     ]
    }
   ],
   "source": [
    "l = labels_and_ids()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97e2759",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model_dir = \"/home/DSE411/Documents/nlp/hf_emotion_classifier/training_scripts/bert/final_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(finetune_model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(finetune_model_dir, num_labels = 6, id2label=l['i2l'], label2id = l['l2i'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33880286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
      "{'sadness': 0, 'joy': 1, 'love': 2, 'anger': 3, 'fear': 4, 'surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)\n",
    "print(model.config.label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "959e8f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:00<00:00, 28923.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "token_data = ds.map(lambda x: tokenize_batch(x, tokenizer), batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b4ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = token_data['train']\n",
    "tokenized_validation = token_data['validation']\n",
    "tokenized_test = token_data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1a493df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "tokenized_train.set_format(type=\"torch\", columns=cols)\n",
    "tokenized_validation.set_format(type=\"torch\", columns=cols)\n",
    "tokenized_test.set_format(type=\"torch\", columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "892b6f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight                        shape=(30522, 768)  dtype=torch.float32\n",
      "bert.embeddings.position_embeddings.weight                    shape=(512, 768)  dtype=torch.float32\n",
      "bert.embeddings.token_type_embeddings.weight                  shape=(2, 768)  dtype=torch.float32\n",
      "bert.embeddings.LayerNorm.weight                              shape=(768,)  dtype=torch.float32\n",
      "bert.embeddings.LayerNorm.bias                                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.0.attention.self.query.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.0.attention.self.query.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.0.attention.self.key.weight                shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.0.attention.self.key.bias                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.0.attention.self.value.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.0.attention.self.value.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.0.attention.output.dense.weight            shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.0.attention.output.dense.bias              shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.0.intermediate.dense.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.0.intermediate.dense.bias                  shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.0.output.dense.weight                      shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.0.output.dense.bias                        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.1.attention.self.query.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.1.attention.self.query.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.1.attention.self.key.weight                shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.1.attention.self.key.bias                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.1.attention.self.value.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.1.attention.self.value.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.1.attention.output.dense.weight            shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.1.attention.output.dense.bias              shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias          shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.1.intermediate.dense.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.1.intermediate.dense.bias                  shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.1.output.dense.weight                      shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.1.output.dense.bias                        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.1.output.LayerNorm.weight                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.1.output.LayerNorm.bias                    shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.2.attention.self.query.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.2.attention.self.query.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.2.attention.self.key.weight                shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.2.attention.self.key.bias                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.2.attention.self.value.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.2.attention.self.value.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.2.attention.output.dense.weight            shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.2.attention.output.dense.bias              shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias          shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.2.intermediate.dense.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.2.intermediate.dense.bias                  shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.2.output.dense.weight                      shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.2.output.dense.bias                        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.2.output.LayerNorm.weight                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.2.output.LayerNorm.bias                    shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.3.attention.self.query.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.3.attention.self.query.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.3.attention.self.key.weight                shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.3.attention.self.key.bias                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.3.attention.self.value.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.3.attention.self.value.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.3.attention.output.dense.weight            shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.3.attention.output.dense.bias              shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias          shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.3.intermediate.dense.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.3.intermediate.dense.bias                  shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.3.output.dense.weight                      shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.3.output.dense.bias                        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.3.output.LayerNorm.weight                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.3.output.LayerNorm.bias                    shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.4.attention.self.query.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.4.attention.self.query.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.4.attention.self.key.weight                shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.4.attention.self.key.bias                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.4.attention.self.value.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.4.attention.self.value.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.4.attention.output.dense.weight            shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.4.attention.output.dense.bias              shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias          shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.4.intermediate.dense.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.4.intermediate.dense.bias                  shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.4.output.dense.weight                      shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.4.output.dense.bias                        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.4.output.LayerNorm.weight                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.4.output.LayerNorm.bias                    shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.5.attention.self.query.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.5.attention.self.query.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.5.attention.self.key.weight                shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.5.attention.self.key.bias                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.5.attention.self.value.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.5.attention.self.value.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.5.attention.output.dense.weight            shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.5.attention.output.dense.bias              shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias          shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.5.intermediate.dense.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.5.intermediate.dense.bias                  shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.5.output.dense.weight                      shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.5.output.dense.bias                        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.5.output.LayerNorm.weight                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.5.output.LayerNorm.bias                    shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.6.attention.self.query.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.6.attention.self.query.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.6.attention.self.key.weight                shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.6.attention.self.key.bias                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.6.attention.self.value.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.6.attention.self.value.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.6.attention.output.dense.weight            shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.6.attention.output.dense.bias              shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias          shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.6.intermediate.dense.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.6.intermediate.dense.bias                  shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.6.output.dense.weight                      shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.6.output.dense.bias                        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.6.output.LayerNorm.weight                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.6.output.LayerNorm.bias                    shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.7.attention.self.query.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.7.attention.self.query.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.7.attention.self.key.weight                shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.7.attention.self.key.bias                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.7.attention.self.value.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.7.attention.self.value.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.7.attention.output.dense.weight            shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.7.attention.output.dense.bias              shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias          shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.7.intermediate.dense.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.7.intermediate.dense.bias                  shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.7.output.dense.weight                      shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.7.output.dense.bias                        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.7.output.LayerNorm.weight                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.7.output.LayerNorm.bias                    shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.8.attention.self.query.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.8.attention.self.query.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.8.attention.self.key.weight                shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.8.attention.self.key.bias                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.8.attention.self.value.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.8.attention.self.value.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.8.attention.output.dense.weight            shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.8.attention.output.dense.bias              shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias          shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.8.intermediate.dense.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.8.intermediate.dense.bias                  shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.8.output.dense.weight                      shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.8.output.dense.bias                        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.8.output.LayerNorm.weight                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.8.output.LayerNorm.bias                    shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.9.attention.self.query.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.9.attention.self.query.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.9.attention.self.key.weight                shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.9.attention.self.key.bias                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.9.attention.self.value.weight              shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.9.attention.self.value.bias                shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.9.attention.output.dense.weight            shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.9.attention.output.dense.bias              shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias          shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.9.intermediate.dense.weight                shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.9.intermediate.dense.bias                  shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.9.output.dense.weight                      shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.9.output.dense.bias                        shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.9.output.LayerNorm.weight                  shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.9.output.LayerNorm.bias                    shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.10.attention.self.query.weight             shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.10.attention.self.query.bias               shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.10.attention.self.key.weight               shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.10.attention.self.key.bias                 shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.10.attention.self.value.weight             shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.10.attention.self.value.bias               shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.10.attention.output.dense.weight           shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.10.attention.output.dense.bias             shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight       shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias         shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.10.intermediate.dense.weight               shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.10.intermediate.dense.bias                 shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.10.output.dense.weight                     shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.10.output.dense.bias                       shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.10.output.LayerNorm.weight                 shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.10.output.LayerNorm.bias                   shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.11.attention.self.query.weight             shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.11.attention.self.query.bias               shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.11.attention.self.key.weight               shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.11.attention.self.key.bias                 shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.11.attention.self.value.weight             shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.11.attention.self.value.bias               shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.11.attention.output.dense.weight           shape=(768, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.11.attention.output.dense.bias             shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight       shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias         shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.11.intermediate.dense.weight               shape=(3072, 768)  dtype=torch.float32\n",
      "bert.encoder.layer.11.intermediate.dense.bias                 shape=(3072,)  dtype=torch.float32\n",
      "bert.encoder.layer.11.output.dense.weight                     shape=(768, 3072)  dtype=torch.float32\n",
      "bert.encoder.layer.11.output.dense.bias                       shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.11.output.LayerNorm.weight                 shape=(768,)  dtype=torch.float32\n",
      "bert.encoder.layer.11.output.LayerNorm.bias                   shape=(768,)  dtype=torch.float32\n",
      "bert.pooler.dense.weight                                      shape=(768, 768)  dtype=torch.float32\n",
      "bert.pooler.dense.bias                                        shape=(768,)  dtype=torch.float32\n",
      "classifier.weight                                             shape=(6, 768)  dtype=torch.float32\n",
      "classifier.bias                                               shape=(6,)  dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "print_model_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b8e874",
   "metadata": {},
   "source": [
    " currently all are in float 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d4f2e",
   "metadata": {},
   "source": [
    "## torchao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d08ec4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53596a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_on_cpu = model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83e35d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2965767/3396547684.py:1: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_model_dynamic = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=6, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model_dynamic = torch.quantization.quantize_dynamic(\n",
    "    model_on_cpu,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "quantized_model_dynamic.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5c5f3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=6, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model_dynamic.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "697eb873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./ptq/tokenizer_config.json',\n",
       " './ptq/special_tokens_map.json',\n",
       " './ptq/vocab.txt',\n",
       " './ptq/added_tokens.json',\n",
       " './ptq/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"./ptq\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "torch.save(quantized_model_dynamic, os.path.join(save_path, \"quantized_model.pt\"))\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a0801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
